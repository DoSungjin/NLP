{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungjindo/NLP/blob/main/FromPretrainedWordEmbeddingsToPretrainedLMRefined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abELyAnBL4Yu"
      },
      "source": [
        "# From Pre-trained Word Embeddings TO Pre-trained Language Models - Focus on Transformers and BERT\n",
        "### based on https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598\n",
        "### The Illustrated Transformers (https://jalammar.github.io/illustrated-transformer/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NmPkkA2L4Yy"
      },
      "source": [
        "![](https://miro.medium.com/max/3900/1*ff_bprXLuTueAx7-5-MHew.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHymFOYXL4Yy"
      },
      "source": [
        "## Static Word Embedding\n",
        "- Skip-Gram & CBOW (aka Word2Vec)\n",
        "- Glove\n",
        "- fastText\n",
        "- Exotic: Lda2Vec, Node2Vec, Characters Embeddings, CNN embeddings, …\n",
        "- Poincaré Embeddings to learn hierarchical representation\n",
        "\n",
        "## Contextualized (Dynamic) Word Embedding (LM)\n",
        "- CoVe (Contextualized Word-Embeddings)\n",
        "- CVT (Cross-View Training)\n",
        "- ELMO (Embeddings from Language Models)\n",
        "- ULMFiT (Universal Language Model Fine-tuning)\n",
        "- BERT (Bidirectional Encoder Representations from Transformers)\n",
        "- GPT & GPT-2 (Generative Pre-Training)\n",
        "- Transformer XL (meaning extra long)\n",
        "- XLNet (Generalized Autoregressive Pre-training)\n",
        "- ENRIE (Enhanced Representation through kNowledge IntEgration)\n",
        "- (FlairEmbeddings (Contextual String Embeddings for Sequence Labelling))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4xSeLyZL4Yz"
      },
      "source": [
        "# What is a good model?\n",
        "Best models would be able to capture 4 components:\n",
        "\n",
        "- Lexical approach (relating to the words or vocabulary of a language)\n",
        "- Syntactic approach (the arrangement of words and phrases to create well-formed sentences in a language -> grammar)\n",
        "- Semantic approach (relating to meaning in language -> stretch the words beyond, understanding ambiguities)\n",
        "- Pragmatic approach (relating proximity between words and documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hLFEchRL4Y0"
      },
      "source": [
        "# What are the main families of models?\n",
        "\n",
        "![](https://miro.medium.com/max/2988/1*X1JSg2zYqD94Mp-MJRBsAw.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKIi5ik5L4Y0"
      },
      "source": [
        "Language modeling is the task of assigning a probability distribution over sequences of words that matches the distribution of a language. Although it sounds formidable, language modeling (i.e. ELMo, BERT, GPT) is essentially just predicting words in a blank. More formally, given a context, a language model predicts the probability of a word occurring in that context.\n",
        "\n",
        "Why is this method effective? Because this method forces the model to learn how to use information from the entire sentence in deducing what words are missing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp9eX1V5L4Y1"
      },
      "source": [
        "# 0. Static vs. Dynamic\n",
        "\n",
        "- Static Word Embeddings fail to capture polysemy. They generate the same embedding for the same word in different contexts. \n",
        "- Contextualized words embeddings aim at capturing word semantics in different contexts to address the issue of polysemous and the context-dependent nature of words.\n",
        "- Static Word Embeddings could only leverage off the vector outputs from unsupervised models for downstream tasks — not the unsupervised models themselves.They were mostly shallow models to begin with and were often discarded after training (e.g. word2vec, Glove) \n",
        "- The output of Contextualized (Dynamic) Word Embedding training is the trained model and vectors — not just vectors.\n",
        "- Traditional word vectors are shallow representations (a single layer of weights, known as embeddings). They only incorporate previous knowledge in the first layer of the model. The rest of the network still needs to be trained from scratch for a new target task. They fail to capture higher-level information that might be even more useful. Word embeddings are useful in only capturing semantic meanings of words but we also need to understand higher level concepts like anaphora, long-term dependencies, agreement, negation, and many more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch4r0tu0L4Y2"
      },
      "source": [
        "## Evolutions\n",
        "![](https://miro.medium.com/max/3212/1*zCoB9_l5NXzlggQikrdxYg.png)\n",
        "![](https://miro.medium.com/max/4000/0*Sl8byJU_nDdyTdMK.png)\n",
        "\n",
        "**Transfer learning** — a technique where instead of training a model from scratch, we use **models pre-trained on a large dataset** and **then fine-tune them for specific natural language tasks.**\n",
        "\n",
        "Some particularities :\n",
        "- ULMFiT → Transfer by Fine Tuning\n",
        "- ELMo → Transfer by Features Extraction\n",
        "- BERT → Transfer by Attention Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oXTT7KRL4Y2"
      },
      "source": [
        "## Why using Transfer Learning?\n",
        "\n",
        "*In vision, it has been in practice for some time now, with people using models trained to learn features from the huge ImageNet dataset, and then training it further on smaller data for different tasks.*\n",
        "\n",
        "- Most datasets for text classification (or any other supervised NLP tasks) are rather small. This makes it very difficult to train deep neural networks, as they would tend to overfit on these small training data and not generalize well in practice.\n",
        "\n",
        "*In computer vision, for a couple of years now, the trend is to pre-train any model on the huge ImageNet corpus. This is much better than a random initialization because the model learns general image features and that learning can then be used in any vision task (say captioning, or detection).*\n",
        "\n",
        "In NLP, we trained on a general language modeling (LM) task and then fine tuned on text classification (or other task). This would, in principle, perform well because the model would be able to use its knowledge of the semantics of language acquired from the generative pre-training.\n",
        "\n",
        "- it is able to capture long-term dependencies in language\n",
        "- it effectively incorporates hierarchical relations\n",
        "- it can help the model learn sentiments\n",
        "- large data corpus is easily available for LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfs_K6PuL4Y3"
      },
      "source": [
        "## The Transformer\n",
        "\n",
        "A model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.\n",
        "\n",
        "The Transformer was proposed in the paper *Attention is All You Need*. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.\n",
        "\n",
        "![Screenshot-from-2019-06-17-19-53-10.webp](attachment:Screenshot-from-2019-06-17-19-53-10.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNhmfa38L4Y3"
      },
      "source": [
        "### A High-Level Look\n",
        "\n",
        "Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/t/the_transformer_3.png\" width=\"60%\">\n",
        "\n",
        "Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Il9KMdEL4Y4"
      },
      "source": [
        "The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXdgUxGDL4Y4"
      },
      "source": [
        "The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/t/Transformer_encoder.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjvsiC7DL4Y4"
      },
      "source": [
        "The encoder’s inputs first flow through a **self-attention layer** – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.\n",
        "\n",
        "The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.\n",
        "\n",
        "The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/t/Transformer_decoder.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMkDFL6kL4Y4"
      },
      "source": [
        "## Job?\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/3092/1*Nm8pVfLjApaGw54-VFUTxg.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xml4Tv-hL4Y5"
      },
      "source": [
        "# From words to Vectors\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1070/1*PPp8BYDRd380rgrWDP42_g.png\" width=\"70%\">\n",
        "\n",
        "- **Tokenization** is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
        "- Using **wordpieces**(e.g. playing -> play + ##ing) instead of words. This is effective in reducing the size of the vocabulary and increases the amount of data that is available for each word.\n",
        "- **Numericalization** aims at mapping each token to a unique integer in the corpus’ vocabulary.\n",
        "- **Token embedding** is the task of get the embedding (i.e. a vector of real numbers) for each word in the sequence. Each word of the sequence is mapped to a emb_dim dimensional vector that the model will learn during training. You can think about it as a vector look-up for each token. The elements of those vectors are treated as model parameters and are optimized with back-propagation just like any other weights.\n",
        "- **Padding** was used to make the input sequences in a batch have the same length. That is, we increase the length of some of the sequences by adding 'pad' tokens.\n",
        "- **Positional encoding**:\n",
        "\n",
        "Recall that the **positional encoding** is designed to help the model learn some notion of **sequences and relative positioning of tokens**. This is crucial for language-based tasks especially here because we are not making use of any traditional recurrent units such as RNN, GRU or LSTM\n",
        "\n",
        "Intuitively, we aim to be able to **modify the represented meaning of a specific word depending on its position**. We don’t want to change the full representation of the word but we want to **modify it a little to encode its position** by adding numbers between [-1,1] using predetermined (non-learned) sinusoidal functions to the token embeddings. For the rest of the **Encoder**, the word will be **represented slightly differently depending on the position the word is in** (even if it is the same word).\n",
        "\n",
        "**Encoder** must be able to use the fact that **some words are in a given position** while, in the same sequence, other words are in other specific positions. That is, we want the network to able to **understand relative positions** and **not only absolute ones.**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/3248/1*OsmkGAkon5IDTwZJ1ORwPA.png\" width=\"80%\">\n",
        "The sinuosidal functions chosen by the authors allow positions to be represented as linear combinations of each other and thus allow the network to learn relative relationships between the token positions.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2980/1*9DD12JPwj1pLY6yUEOv35A.png\" width=\"80%\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2980/1*Y1MDr4WgzYp4eZaBOuJIYw.png\" width=\"80%\">\n",
        "\n",
        "**Positional embeddings** could be understood as the **distance between different words in the sequence**. The intuition here is that adding these values to the embeddings provides **meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/3228/1*Q2MAvQ4CuhyPaXBDuNyEKg.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOht4IrVL4Y5"
      },
      "source": [
        "# Encoder Blocks\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1824/1*6j1Fn5sI12Qnv-pL9M57ZQ.png\" width=\"60%\">\n",
        "A total of N encoder blocks are chained together to generate the **Encoder’s** output.\n",
        "\n",
        "Note: In BERT’s experiments, the number of blocks N (or L, as they call it) was chosen to be 12 and 24.\n",
        "\n",
        "- The dimensions of the **input** and **output** of the encoder block are the same. Hence, it makes sense to **use the output of one encoder block as the input of the next encoder block**.\n",
        "- A specific block is in charge of **finding relationships between the input representations and encode them** in its output.\n",
        "- The blocks do not share weights with each other.\n",
        "- This iterative process through the blocks will help the neural network capture more complex relationships between words in the input sequence.\n",
        "- The **Transformer** uses **Multi-Head Attention**, which means it computes attention **h different times** with **different weight matrices** and then concatenates the results together.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2830/0*FHV50Xi3zCD4_76k.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mncLlS7aL4Y6"
      },
      "source": [
        "## Attention Mechanism\n",
        "\n",
        "Let’s dive into attention mechanism. Note that Multi Head Self Attention is different between **ENCODER block** and **DECODER block**.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/3216/1*RAOLaGJ5gBEWUmTjPXmA4Q.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_DjNXAiL4Y6"
      },
      "source": [
        "## A - One Head Self-Attention\n",
        "\n",
        "A **RNN** maintains a hidden state allows it to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. **Self-attention** is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n",
        "\n",
        "![](https://miro.medium.com/max/3076/1*vBhpTGfzaOTAG5iIAHTcxg.png)\n",
        "\n",
        "What does “it” in this sentence refer to? Is it referring to the street or to the animal?\n",
        "\n",
        "- Create **three vectors** from each of the **encoder’s input vectors**\n",
        "- For **each word**, we create **a Query vector, a Key vector, and a Value vector\n",
        "- These vectors are created by **multiplying the embedding** by **three matrices** that we trained during the training process.\n",
        "\n",
        "Notice that **these new vectors** are **smaller in dimension than the embedding vector**. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512.\n",
        "\n",
        "Why **dimensionality is 64?** As we must have :\n",
        "\n",
        "-> Output’s dimension is [length of input sequences] x [dimension of embeddings — 512]\n",
        "\n",
        "-> We use 8 heads during Multi-head Self-Attention process. The output size of a given self attention vector is [length of input sequences] x [64]. So the concatenated vector resulting from all Multi-head Self-Attention process would be [length of input sequences] x ([64] x [8]) = [length of input sequences] x ([512])\n",
        "\n",
        "![](https://miro.medium.com/max/2616/1*ic6fUK2W86NL0igIjHAPZQ.png)\n",
        "\n",
        "**Query q**: the query vector q encodes the word/position on the left that is paying attention, i.e. the one that is “querying” the other words. In the example above, the query vector for “the” (the selected word) is highlighted.\n",
        "\n",
        "**Key k**: the key vector k encodes the word on the right to which attention is being paid. The key vector together with the query vector determine the attention score between the respective words, as described below.\n",
        "\n",
        "**q×k (element-wise)**: the element-wise product of the query vector and a key vector. This product is computed between the selected query vector and each of the key vectors. This is a precursor to the dot product (the sum of the element-wise product) and is included for visualization purposes because it shows how individual elements in the query and key vectors contribute to the dot product.\n",
        "\n",
        "**q·k**: the dot product of the selected query vector and each of the keyvectors. This is the unnormalized attention score.\n",
        "\n",
        "**Softmax**: the softmax of q·k / 8 across all target words. This normalizes the attention scores to be positive and sum to one. The constant factor 8 is the square root of the vector length (64). This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\n",
        "How to calculate these three vectors?\n",
        "\n",
        "![](https://miro.medium.com/max/2652/1*5rKwra-kv9DUpwGbZ-NYVA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBDT4ndsL4Y6"
      },
      "source": [
        "We calculate self-attention for every word in the input sequence.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2136/1*etLG-EqfKl5GcPxne9IQBQ.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFn8XjYXL4Y6"
      },
      "source": [
        "## Focus on \"Score\"\n",
        "\n",
        "*[Q x Transpose of K] is a product scalar between **Query vector** and **Key Vector**. The closer the **Key Vector** is than the Query Vector, the higher the score — resulting from [Q x Transpose of K]- would be.*\n",
        "\n",
        "*Softmax will provide us a probability distribution which keeps increasing the value for key vectors which are similar to respective query vectors and consequently keep decreasing key vectors which are far away from query vector.*\n",
        "\n",
        "- **d_k** and **d_v** are set such that d_k = d_v = emb_dim/h.\n",
        "\n",
        "Remember that **Q** and **K** were different projections of the tokens into a **d_k (i.e. 64) dimensional space**. Therefore, we can think about **the dot product of those projections as a measure of similarity between tokens projections**. For every vector projected through Q the dot product with the projections through K measures the similarity between those vectors. If we call v_i and u_j the projections of the i-th token and the j-th token through Q and K respectively, their dot product can be seen as:\n",
        "\n",
        "![](https://miro.medium.com/max/414/1*aGrtfwdGBG3D5SCbnB-XlA.png)\n",
        "\n",
        "This is a measure of **how similar are the directions of u_i and v_j** and **how large are their lengths** (the closest the direction and the larger the length, the greater the dot product).\n",
        "\n",
        "Another way of thinking about this matrix product is as the encoding of a **specific relationship between each of the tokens in the input sequence**(the relationship is defined by the matrices K, Q).\n",
        "![](https://miro.medium.com/max/2418/0*XoCOJ5GzlqIa62LG.png)\n",
        "\n",
        "![](https://miro.medium.com/max/4104/1*se1wlg03TiR3bYnyI8q9PA.png)\n",
        "(https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea95zvcQL4Y7"
      },
      "source": [
        "We see that the **product of the query vector** for “the” and **the key vector** for “store” (the next word) is strongly positive across most neurons. For tokens other than the next token, the **key-query product contains some combination of positive and negative values**. The result is a high attention score between “the” and “store”.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2568/1*2aZiyXVnZTVgAWxvWMjzyQ.png\" width=\"80%\">\n",
        "\n",
        "**Example**: Consider this phrase — “Action gets results”. To calculate the self-attention for the first word “Action”, we will **calculate scores for all the words in the phrase with respect to “Action”. This score determines the importance of other words when we are encoding a certain word in an input sequence.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/3336/1*yw2PxgzsjNNKE-UaVoZESQ.png\" width=\"80%\">\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/2976/1*R9NE3rNKGO1ThN2jhZYBRg.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL6BqrF9L4Y7"
      },
      "source": [
        "Let's go back with our initial example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro7bDgrKL4Y7"
      },
      "source": [
        "\n",
        "<img src=\"https://miro.medium.com/max/1648/1*tbb9rywOeo3kBtJ85ZdPFA.png\" width=\"80%\">\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1648/1*mllxBXok93AsQ_m43D3v_g.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVqm2lML4Y7"
      },
      "source": [
        "To sum up when it comes to One-Head Self Attention\n",
        "\n",
        "The main idea behind attention is **lookup-table**, a table that has a large number of values for some other values and **you ask it a query** and **it returns one closest to it**. In the method used here, we feed it three values, key, value and query. There are large number of keys, basically 1-dimensional vectors in n-dimensional space, where each key has some corresponding value.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1989/1*yA0mAyNVnbSSR7vFZb87ow.png\" width=\"60%\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/666/1*AsRQcLJ1y7U0wVdj75Vpzw.png\" width=\"60%\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1112/1*9grAnDs4hhtwPtx3axbC_Q.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJd-0X25L4Y8"
      },
      "source": [
        "## B-Muti-Head Self-Attention\n",
        "\n",
        "The paper notes that **“additive attention”** performs better than the self attention described above, though it is much slower. Additive attention uses a more complicated compatibility function — namely a feed forward neural network.\n",
        "\n",
        "Self-attention is computed not once but multiple times in the Transformer’s architecture, in parallel and independently. It is therefore referred to as Multi-head Attention. The outputs are concatenated and linearly transformed as shown in the figure below:\n",
        "\n",
        "![](https://miro.medium.com/max/240/0*JVR9ZEXkq3LH5_c_.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExWY-zXYL4Y8"
      },
      "source": [
        "The Transformer uses **eight attention heads**, so we end up with **eight sets for each encoder/decoder**. Each set is used to **project the input embeddings into a different representation subspace**. If we do the same self-attention calculation we described just above, we end up with eight different Z matrices.\n",
        "\n",
        "However, **the feed-forward layer is not expecting eight matrices**. We need to concatenate them and condense these eight down into a single matrix by multiply them with an additional weights matrix WO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhwtSPD0L4Y8"
      },
      "source": [
        "How to return 8 matrices Z1…Z8 into a singe matrix Z in Multi-head attention?\n",
        "\n",
        "![](https://miro.medium.com/max/1384/1*APB6M-aibDcXSBNxF3A6dQ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tu9isewL4Y8"
      },
      "source": [
        "To sum up...\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1668/1*WWMs-xd5zUiLB6n6haoETQ.png\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh74Lgd5L4Y8"
      },
      "source": [
        "Here is the result :\n",
        "<img src=\"https://miro.medium.com/max/1438/1*9Fe9bexZEzVesIKRL2V4hQ.gif\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAM1UnZzL4Y8"
      },
      "source": [
        "# Dropout, Add & Norm\n",
        "\n",
        "https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture12.pdf\n",
        "\n",
        "Before this layer, there is always a layer for which inputs and outputs have the same dimensions (*Multi-Head Attention or Feed-Forward*). We will call that layer *Sublayer* and its input x.\n",
        "\n",
        "After each *Sublayer*, dropout is applied with 10% probability. Call this result *Dropout(Sublayer(x))*. This result is added to the Sublayer’s input x, and we get x + *Dropout(Sublayer(x))*.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/666/0*e_3Pv_3pT5O4WHJb.png\" width=\"50%\">\n",
        "\n",
        "Observe that in the context of a *Multi-Head Attention* layer, this means **adding the original representation of a token x to the representation based on the relationship with other tokens**. It is like telling the token:\n",
        "\n",
        "**“Learn the relationship with the rest of the tokens, but don’t forget what we already learned about yourself!”**\n",
        "\n",
        "Finally, a token-wise/row-wise normalization is computed with the mean and standard deviation of each row. This improves the stability of the network.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/712/0*k9KOvRpb3Lk-58W4.png\" width=\"50%\">\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1766/1*udWf0jTd3DyrNMvzOafs0Q.png\" width=\"60%\">\n",
        "\n",
        "We compute **the mean and variance used for normalization** from all of the summed inputs to the neurons in a layer on a single training case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPzPg78nL4Y8"
      },
      "source": [
        "# Position-wise-Feed-Forward Network\n",
        "\n",
        "In addition to attention sub-layers, each of the layers in our encoder and decoder contains **a fully connected feed-forward network**, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "![](https://miro.medium.com/proxy/1*y2lQq8XHC0E373OxwIM6Ow.png)\n",
        "(https://mc.ai/seq2seq-pay-attention-to-self-attention-part-2/)\n",
        "\n",
        "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512, and the inner-layer has dimensionality dff=2048.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e14y_kN0L4Y9"
      },
      "source": [
        "# 3. Decoder Block\n",
        "\n",
        "Each **decoder layer** consists of sublayers:\n",
        "\n",
        "1. Masked multi-head attention (with **look ahead mask and padding mask**)\n",
        "2. Multi-head attention (with padding mask). **V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from *the masked multi-head attention sublayer.**\n",
        "3. Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)).\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As **Q** receives **the output from decoder’s first attention block**, and **K** receives the **encoder output**, **the attention weights represent the importance given to the decoder’s input based on the encoder’s output**. In other words, **the decoder predicts the next word by looking at the encoder output and self-attending to its own output**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0x8ECZKL4Y9"
      },
      "source": [
        "## Multi Head Masked Self Attention\n",
        "\n",
        "In *encoder*, self-attention layers process input **queries, keys and values** that comes from the **output of previous layer**. Each position in encoder can attend to all positions from previous layer of the encoder.\n",
        "\n",
        "In *decoder*, self-attention layer enable **each position to attend to all previous positions in the decoder**, including the current position.\n",
        "\n",
        "![](https://miro.medium.com/max/338/1*sgJkbZyociQQnDmSmkdk0Q.png)\n",
        "(https://persagen.com/resources/biokdd-review-nlu.html)\n",
        "\n",
        "![](https://miro.medium.com/max/507/1*4olcOVVPJp0648VVuD-HGg.png)\n",
        "\n",
        "In other words, the self-attention layer is only allowed to attend to **earlier positions in the output sequence**. Masking multi-head attention is done by masking future positions (setting them to -∞) before the softmax step in the self-attention calculation. **This step ensures that the predictions for position i can depend only on the known outputs at positions less than *i*. Since we want these elements to be zero after the softmax, we set them to −∞."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNC5-YgML4Y9"
      },
      "source": [
        "![](https://miro.medium.com/max/451/1*lJvMNS3bpOSsvohmJkgx3w.png)\n",
        "\n",
        "With RNNs — there is no issue like that, since **they cannot look forward into the input sequence: output i depends only on inputs 0 to i**. With a transformer, the output depends on the entire input sequence, so prediction of the next words/characters becomes vacuously easy, just retrieve it from the input.\n",
        "\n",
        "To use self-attention as an autoregressive model, we’ll need to ensure that it **cannot look forward into the sequence**. We do this by **applying a mask to the matrix of dot products**, before the softmax is applied. **This mask disables all elements above the diagonal of the matrix**.\n",
        "\n",
        "After we’ve **handicapped the self-attention module like this**, the model **can no longer look forward in the sequence**.\n",
        "\n",
        "The “**Decoder Attention**” layer works just like multiheaded self-attention, except it creates its **Queries matrix** from the **layer below it**, and **takes the Keys and Values matrix from the output of the encoder stack**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yyzPFnRL4Y9"
      },
      "source": [
        "![](https://miro.medium.com/max/1342/0*35MlGUV6krab7BSZ.png)\n",
        "![](https://miro.medium.com/max/684/1*e-zIdj7U1IKzy2ttj_Zd-g.png)\n",
        "\n",
        "Self-attention layers in the decoder allow **each position in the decoder to attend to all positions in the decoder up to and including that position**. We need to prevent **leftward information flow** in the decoder to preserve the **auto-regressive property**. We implement this inside of scaled dot- product attention by masking out (setting to −∞) **all values in the input of the softmax which correspond to illegal connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCIvCbwhL4Y9"
      },
      "source": [
        "# 4. The Final Linear and Softmax Layer\n",
        "\n",
        "The decoder stack outputs **a vector of floats. How do we turn that into a word?** That’s the job of the final Linear layer which is followed by a Softmax Layer.\n",
        "\n",
        "**The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector**. *This space is the size of vocabulary (all words). We just project the matrix of weights (provided by the decoder block) into a “vocabulary space”.\n",
        "\n",
        "Mathematically speaking, what does it mean?\n",
        "\n",
        "Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would **make the logits vector 10,000 cells wide — each cell corresponding to the score of a unique word**. That is how we interpret the output of the model followed by the Linear layer.\n",
        "\n",
        "The **softmax layer** then **turns those scores into probabilities** (all positive, all add up to 1.0). **The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.** Softmax provides us **the most likely word to predict** (we take the word of the column which give us the highest probability).\n",
        "\n",
        "![](https://miro.medium.com/max/869/0*YJ6y9N3jvQQHbjxl.png)\n",
        "\n",
        "This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwbJHS0aL4Y-"
      },
      "source": [
        "# 5. Residual Connection\n",
        "\n",
        "A residual connection is basically just taking the input and adding it to the output of the sub-network, making training deep networks easier in the field of computer vision. Layer normalization is a normalization method in deep learning that is similar to batch normalization. In layer normalization, the statistics are computed across each feature and are **independent of other examples**. The independence between inputs means that each input has a different normalization operation.\n",
        "\n",
        "![](https://miro.medium.com/proxy/1*z-jPITfcAcgijEMaizhrZw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO2YOLvtL4Y-"
      },
      "source": [
        "# 7. Model Training - How BERT is trained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldak5SlPL4Y-"
      },
      "source": [
        "## BERT\n",
        "\n",
        "BERT is basically a trained Transformer Encoder stack. \n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-transfer-learning.png\" width=\"60%\">\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-base-bert-large-encoders.png\" width=\"60%\">\n",
        "\n",
        "Both BERT model sizes have a large number of encoder layers (which the paper calls Transformer Blocks) – twelve for the Base version, and twenty four for the Large version. These also have larger feedforward-networks **(768 and 1024 hidden units respectively)**, and more attention heads **(12 and 16 respectively)** than the default configuration in the reference implementation of the Transformer in the initial paper **(6 encoder layers, 512 hidden units, and 8 attention heads).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRPK3jULL4Y-"
      },
      "source": [
        "### Model Inputs\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-input-output.png\" width=\"60%\">\n",
        "\n",
        "The first input token is supplied with a special [CLS] token for reasons that will become apparent later on. CLS here stands for Classification.\n",
        "\n",
        "Just like the vanilla encoder of the transformer, BERT takes a sequence of words as input which keep flowing up the stack. Each layer applies self-attention, and passes its results through a feed-forward network, and then hands it off to the next encoder.\n",
        "\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-encoders-input.png\" width=\"60%\">\n",
        "In terms of architecture, this has been identical to the Transformer up until this point (aside from size, which are just configurations we can set). It is at the output that we first start seeing how things diverge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY-6fOqXL4Y-"
      },
      "source": [
        "### Model Outputs\n",
        "\n",
        "Each position outputs a vector of size hidden_size (768 in BERT Base). For the sentence classification example we’ve looked at above, we focus on the output of only the first position (that we passed the special [CLS] token to).\n",
        "<img src=\"http://jalammar.github.io/images/bert-output-vector.png\" width=\"60%\">\n",
        "That vector can now be used as the input for a classifier of our choosing. The paper achieves great results by just using a single-layer neural network as the classifier.\n",
        "\n",
        "<img src=\"http://jalammar.github.io/images/bert-classifier.png\" width=\"60%\">\n",
        "If you have more labels (for example if you’re an email service that tags emails with “spam”, “not spam”, “social”, and “promotion”), you just tweak the classifier network to have more output neurons that then pass through softmax.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSdviwUL4Y-"
      },
      "source": [
        "### Improvements\n",
        "![](https://miro.medium.com/max/1234/1*KbAUVetHPMreJdcbicmJrw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPFAvBJQL4Y-"
      },
      "source": [
        "1. Differences between GPT vs. ELMo vs. BERT -> all pre-training model architectures. BERT uses a bidirectional Transformer vs. GPT uses a left-to-right Transformer vs. ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTM to generate features for downstream task. BERT representations are jointly conditioned on both left and right context in all layers. In other words, it is deeply bidirectional, as opposed to ELMo (shallow bidirectional) and OpenAI GPT (one direction, left to right).\n",
        "2. Transformers demonstrate that recurrence and convolution are not essential for building high-performance natural language models\n",
        "3. They achieve state-of-the-art machine translation results using a self-attention operation\n",
        "4. Attention is a highly-efficient operation due to its parallelizability and runtime characteristics\n",
        "5. Traditional language models take the previous n tokens and predict the next one. In contrast, BERT trains a language model that takes both the previous and next tokens into account when predicting- really bidirectional.\n",
        "6. If you simply ask a deep neural network to learn what typical English sentences look like by reading all of Wikipedia, what does it learn about the English language? BERT encode human-like parse trees and find tree structures in these vector spaces when computer have represented each word in the sentence as a real-valued vector, with no explicit representation of the parse tree. BERT has the ability to reconstruct parse trees from the Penn Treebank.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1984/0*1GqiGkk0LW6hMLsA.png\" width=\"60%\">\n",
        "- Scoring the importance of each layer for a specific NLP task (e.g. POS, NER etc.), shows basic syntactic information is captured earlier (lower layers) in the network followed by semantic information in higher layers. This reflected in the image on the right. (This observation is similar to what was seen in ELMo model too).\n",
        "- Also, information related to syntactic tasks seems to be more localized in few layers, where information for semantic tasks (SPR and Relations) is generally spread across the entire network\n",
        "- Examining the output word vectors show not only are different senses of a word captured in distinct representations they are also spatially separated in a fine grained manner.\n",
        "\n",
        "\n",
        "\n",
        "7. **Distance each word has to travel**: In a simple RNN, the word ‘Echt’ has to travel multiple steps. The last red layer has to store the encoded information. In large sentences which are over 50 words long, **the amount of distance each word has to travel increases linearly.** And since we keep writing over that encoded information, we are sure to loose important words that come early in the sentence. After encoding it also h as to travel to get to it’s decoded destination.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1000/1*kl2ldmXEFCxVtvDVsDePug.png\" width=\"60%\">\n",
        "\n",
        "With an **attention mechanism** we no longer try encode the full source sentence **into a fixed-length vector.** Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation.\n",
        "\n",
        "The significant achievement of attention mechanism was to improve **spatial understanding of the model.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcs1WD9FL4Y_"
      },
      "source": [
        "\n",
        "## A - Masked Language Modeling (MLM)\n",
        "\n",
        "*“The masked language model randomly masks some of the tokens from the input, and the objective is to **predict the original vocabulary id of the masked word based only on its context**. Unlike **left-to-right language model pre-training**, the MLM objective allows the representation to fuse the left and the right context, which allows us to pre-train **a deep bidirectional Transformer.”*\n",
        "\n",
        "The Google AI researchers masked **15% of the words in each sequence at random.** The task? To predict these masked words. A caveat here — the masked words were not always replaced by the masked tokens [MASK] because the [MASK] token would never appear during fine-tuning.\n",
        "\n",
        "So, the researchers used the below technique:\n",
        "- 80% of the time the words were replaced with the masked token [MASK]\n",
        "- 10% of the time the words were replaced with random words\n",
        "- 10% of the time the words were left unchanged\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1600/0*7n-KcrsCy3WrrmUb\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWGY5RfL4Y_"
      },
      "source": [
        "## B- Next Sentence Prediction\n",
        "\n",
        "Generally, language models do not capture the relationship between consecutive sentences. BERT was pre-trained on this task as well.\n",
        "\n",
        "For language model pre-training, BERT uses pairs of sentences as its training data. The selection of sentences for each pair is quite interesting. Let’s try to understand it with the help of an example.\n",
        "\n",
        "Imagine we have a text dataset of 100,000 sentences and we want to pre-train a BERT language model using this dataset. So, there will be 50,000 training examples or pairs of sentences as the training data.\n",
        "\n",
        "- For 50% of the pairs, the second sentence would actually be the next sentence to the first sentence\n",
        "- For the remaining 50% of the pairs, the second sentence would be a random sentence from the corpus\n",
        "- The labels for the first case would be ‘IsNext’ and ‘NotNext’ for the second case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKLRJWwoL4Y_"
      },
      "source": [
        "# 8. Pre-trained Models\n",
        "\n",
        "\n",
        "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S2666651021000231-gr9.jpg\" width=\"80%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISzFKuh_L4Y_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "FromPretrainedWordEmbeddingsToPretrainedLMRefined.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}